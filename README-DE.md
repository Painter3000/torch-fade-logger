# torch-fade-logger
Runtime GPU Validator for PyTorch on ZLUDA/ROCm

* Linux Mint 21.2 - Victoria / Ubuntu 22.04 LTS 
* Hardware-Referenz (RDNA2 RX 6800 XT) 
* Docker-Workflow inkl. Nachinstallation der Dev-Pakete 
* richtigen Ablauf zwischen `build_amd.py` und Logger-Patch 
* CMake-Anpassung in `build_rocm_v2.9.sh`
---

ğŸ“‚ Logger-Verzeichnisstruktur:
```bash
./workspace/
    â”œâ”€â”€ Dockerfile
    â””â”€â”€ pytorch
          â”œâ”€â”€ build_rocm_v2.9.sh
          â””â”€â”€ fade_hip_logger_full_setup.sh
```
ğŸ’¡ Alle benÃ¶tigten Dateien, sowie dazugehÃ¶rige Ordner werden mittels Skript erzeugt!

--- 

# ğŸ”¦ FADELogger â€“ Runtime-GPU-Validierung fÃ¼r ZLUDA, ROCm & PyTorch 

> Getestet auf: AMD Radeon RX 6800 XT (RDNA2) unter Docker mit ROCm 6.4.x 
> UnterstÃ¼tzt Torch 2.9 Dev (Branch `main`, Stand: Juli 2025) 

# ğŸ¯ Motivation 

PyTorch unterstÃ¼tzt AMD-GPUs selbst bei der Version 2.9 **nur rudimentÃ¤r bzw. gar nicht** â€“ GPU-Fallbacks zur CPU sind leider hÃ¤ufig die Regel. 
**ZLUDA**, ursprÃ¼nglich fÃ¼r Intel entwickelt, simuliert CUDA auf AMD Ã¼ber HIP â€“ bietet also eine MÃ¶glichkeit, Torch â€GPU-UnterstÃ¼tzungâ€œ als Nvidia-GPU vorzutÃ¤uschen. 


Doch diese Simulation hat TÃ¼cken: 

ğŸ§  Eine RX 6800 XT besitzt z.â€¯B. **72 Compute Units**, ZLUDA erkennt jedoch nur **36**. 
Diese falschen Device-Werte fÃ¼hren zu verzerrter Performance und fehlerhafter Ressourcenallokation â€“ ohne dass der Entwickler es merkt. 

Der **FADELogger** setzt genau hier an: 

- Er dokumentiert HIP- und Torch-Aufrufe **zur Laufzeit** 
- Er erkennt falsche GPU-Metadaten â€“ wie CU-Zahl, Taktung, Architektur 
- Er bietet die Option, korrigierte Werte zurÃ¼ckzugeben 
  ğŸ‘‰ z.â€¯B. `ComputeUnits=72` statt `36` bei RX 6800 XT 

Damit wird nicht nur sichtbar, ob Torch **angeblich** auf einer CUDA-GPU lÃ¤uft â€“ sondern auch **wie korrekt** die zugrunde liegende Symbol-Logik funktioniert. 

ğŸ’¡ Das Ergebnis ist keine bloÃŸe Log-Datei, sondern ein Werkzeug zur aktiven Verifikation und Korrektur der GPU-Schicht auf RDNA2-Systemen. 

--- 

## ğŸ“¦ Docker-Setup fÃ¼r FADELogger (RDNA2)

> Getestet auf: AMD Radeon RX 6800 XT (gfx1030), ROCm 6.4.x, Ubuntu 22.04 Das folgende Dockerfile erzeugt eine vollstÃ¤ndige Entwicklungsumgebung fÃ¼r:

* ğŸ§± Build von PyTorch 2.9 mit ROCm
* ğŸ”¬ FADELogger-Integration (C++ + Python)
* âš™ï¸ HIP Runtime Debugging auf RDNA2

### ğŸ”§ Dockerfile (Beispiel)
```dockerfile
# PyTorch ROCm Build Container fÃ¼r AMD RX 6800 XT
FROM rocm/dev-ubuntu-22.04:latest

# Umgebungsvariablen setzen
ENV DEBIAN_FRONTEND=noninteractive
ENV PYTORCH_ROCM_ARCH=gfx1030
ENV USE_ROCM=1
ENV USE_CUDA=0
ENV MAX_JOBS=4

# System-Updates und AbhÃ¤ngigkeiten installieren
RUN apt-get update && apt-get install -y \
  apt-utils \
  hipsolver-dev \
  rocthrust-dev \
  hipcub-dev \
  rocprim-dev \
  hipsparse-dev \
  hipfft-dev \
  hipblas-dev \
  rocblas-dev \
  miopen-hip-dev \
  hiprand-dev \
  rocrand-dev \
  rccl-dev \
  rocfft-dev \
  python3 \
  python3-pip \
  python3-dev \
  python3-setuptools \
  git cmake ninja-build build-essential \
  libopenblas-dev \
  libjpeg-dev \
  libpng-dev \
  libtiff-dev \
  libavcodec-dev \
  libavformat-dev \
  libswscale-dev \
  libv4l-dev \
  libatlas-base-dev \
  gfortran pkg-config \
  software-properties-common wget curl \
  && rm -rf /var/lib/apt/lists/* 

RUN pip3 install --upgrade pip setuptools wheel
RUN pip3 install \
  numpy \
  pyyaml \
  typing_extensions \
  requests \
  future \
  six \
  cmake>=3.27 \
  ninja \
  packaging

WORKDIR /workspace
ENV PATH="/opt/rocm/bin:${PATH}"
ENV LD_LIBRARY_PATH="/opt/rocm/lib:${LD_LIBRARY_PATH}"
CMD ["/bin/bash"]
```

---


### â–¶ï¸ Container mit Dockerfile erzeugen (lokal)
```bash
docker build -t pytorch-fade .
```

### â–¶ï¸ Container mit Parameter starten (lokal)
```bash
docker run -it \
  --device=/dev/kfd \
  --device=/dev/dri \
  --group-add video \
  --cap-add=SYS_ADMIN \
  --cap-add=SYS_PTRACE \
  --security-opt seccomp=unconfined \
  --ipc=host \
  -v /home/oem/Programme/docker/FADE:/workspace \
  pytorch-rocm-build
``` 

Damit wird der Container nicht nur einfach gestartet, sondern bekommt gezielt Zugriffe und Rechte, die fÃ¼r Hardwarebeschleunigung (z.â€¯B. Ã¼ber ROCm) und Debugging wichtig sind. Hier ist, was die einzelnen Parameter bewirken:  

ğŸ§© **Parameter-ErklÃ¤rung**:

| Parameter                         | Zweck                                                                 |
|----------------------------------|------------------------------------------------------------------------|
| `--device=/dev/kfd`              | Zugriff auf die ROCm-Komponente fÃ¼r Kernel Fusion Dispatcher          |
| `--device=/dev/dri`              | ErmÃ¶glicht Zugriff auf Direct Rendering Infrastructure (GPU-Zugriff)  |
| `--group-add video`              | FÃ¼gt den Container der Video-Gruppe hinzu â€“ nÃ¶tig fÃ¼r GPU-Nutzung     |
| `--cap-add=SYS_ADMIN`            | Erlaubt administrative Funktionen, z.â€¯B. fÃ¼r Debugging oder Mounts     |
| `--cap-add=SYS_PTRACE`           | ErmÃ¶glicht Debugging im Container durch `ptrace`                      |
| `--security-opt seccomp=unconfined` | Hebt die standardmÃ¤ÃŸige Syscall-Filterung auf â€“ fÃ¼r mehr FlexibilitÃ¤t |
| `--ipc=host`                     | Container teilt IPC-Speicher mit dem Host â€“ nÃ¼tzlich fÃ¼r PyTorch      |
| `-v /home/...:/workspace`        | Bind-Mount: Host-Verzeichnis wird im Container eingebunden            |
| `-it`                            | Interaktives Terminal                                                 |
| `pytorch-rocm-build`             | Das verwendete Image                                                  |

âœ¨ Dadurch wird der Container **performancefÃ¤hig fÃ¼r GPU-gestÃ¼tzte Deep Learning-Anwendungen** wie FADE â€“ besonders wenn du mit ROCm arbeitest.  

### ğŸ“Œ Hinweise 

* **Entwicklerpakete (dev)** wie `rocblas-dev`, `hipfft-dev` usw. mÃ¼ssen **zwingend vorhanden sein**, sonst scheitert der Torch-Build! 
* `PYTORCH_ROCM_ARCH=gfx1030` ist angepasst fÃ¼r **RX 6800 XT (RDNA2)**. FÃ¼r andere Karten ggf. `gfx1031`, `gfx1100`, etc. wÃ¤hlen. 
* Optional kann im Dockerfile direkt `git clone https://github.com/pytorch/pytorch` eingefÃ¼gt werden â€“ aber besser manuell ausfÃ¼hrt, falls Forks oder Branches genutzt werden. 

ğŸ’¡NÃ¼tzliche Docker- Befehle aus Host-Terminal: 
```bash 
docker ps -a
```
- Container auflisten (alle)

```bash 
docker start -ai <CONTAINER-NAME>
```
- starten des Docker-Containers z.B. docker start -ai tender_mahavira
--- 

### 2. Dev-Pakete **nachtrÃ¤glich installieren** bei Bedarf

Das ROCm-Docker-Image enthÃ¤lt manch mal nicht alle nÃ¶tigen Development-Pakete, obwohl diese zuvor im Dockerfile angegeben wurden! 

```bash 
apt-get update && apt-get install -y \
  hiprand-dev rocblas-dev miopen-hip-dev \
  rocrand-dev rccl-dev rocfft-dev \
  hipblas-dev hipfft-dev hipsparse-dev \
  rocprim-dev hipcub-dev rocthrust-dev \
  hipsolver-dev hipsparselt-dev
```

---

## âš™ï¸ PyTorch (2.9 Dev) Setup

### 1. Klonen
```bash
git clone --recursive https://github.com/pytorch/pytorch
cd pytorch
git submodule sync
git submodule update --init --recursive
``` 
### 2. ROCm-spezifische HIP-Dateien generieren
```bash
python3 tools/amd_build/build_amd.py
```
> âš ï¸ **Wichtig:** Dieses Skript erzeugt HIP-spezifische Dateien (u.â€¯a. `HIPGuardImplMasqueradingAsCUDA.*`), 
> die fÃ¼r die FADELogger-Integration zwingend notwendig sind! 
> Wer direkt mit dem Logger-Patch beginnt, wird viele Stellen 
**nicht instrumentiert bekommen**. 

--- 

## ğŸš€ FADELogger installieren 
```bash 
./fade_hip_logger_full_setup.sh 
``` 

**Was passiert dabei?** 

* ğŸ“„ Logger-Dateien (`fade_logger.cpp`, Header) werden erstellt 
* ğŸ§© CMakeLists.txt wird automatisch erweitert 
* ğŸ§  Instrumentierung fÃ¼r `hipMalloc`, `hipLaunchKernel`, `hipMemcpy`, `hipStreamSynchronize`, ... 
* ğŸ” Optional: Log-Auswertungsskript `fade_log_analyzer.py` wird erzeugt 

--- 


## ğŸ› ï¸ Build mit ROCm 
```bash 
./build_rocm_v2.9.sh 
``` 

> ğŸ“ Passe in dieser Datei (`build_rocm_v2.9.sh`) bei Bedarf: 
> 
> * die Pfade zu ROCm (z.â€¯B. `--rocm-path`) 
> * die GPU-Architektur (`gfx1030` o.â€¯Ã¤.) 
> * und weitere CMake-Flags an 

Nach erfolgreichem Build prÃ¼fen: 
```bash 
pip list | grep torch # â†’ torch 2.9.0a0+gXXXXX /workspace/pytorch 
``` 

--- 

## ğŸ§ª FADELogger ausfÃ¼hren 
```bash 
python3 fade_log_analyzer.py 

``` 

Beispielausgabe: 

```
ğŸ§  FADE PyTorch Debug-Analyse 
============================= 
ğŸ“Š Gesamt-Events: 44 
ğŸ¯ Aktive GerÃ¤te: {0: 1} 

ğŸ“‹ Top Funktionsaufrufe: 
REDIRECT: 32 
DEVICE: 8 
TEST: 2 
STREAM: 1
```

---

## ğŸ“Š Feature-Ãœbersicht 

| Kategorie | Beschreibung | 
| --------------- | --------------------------------------------------------------------------- | 
| ğŸ¯ Device-Check | Loggt `hipGetDeviceProperties`, CU-Anzahl, Architektur, Taktrate | 
| ğŸ”„ Redirects | Erkennt CUDAâ†’HIP-Umleitungen (Symbol-Interception, ZLUDA-Bypass) | 
| ğŸ§  Funktionslog | Dokumentiert Aufrufe wie `hipLaunchKernel`, `hipMemcpy`, `hipMalloc` | 
| ğŸ§¾ JSONL-Format | Zeitstempel, Funktion, Argumente, Ergebnis â€“ ideal fÃ¼r automatische Analyse | 

--- 

## âš ï¸ Warum FADELogger? 

* PyTorch 2.9 erkennt ROCm/HIP oft **nicht korrekt**, obwohl `libamdhip64.so` geladen ist 
* ZLUDA kann **DeviceProperties faken** 
* Torch tendiert zu **silent Fallbacks** â†’ es lÃ¤uft plÃ¶tzlich auf CPU 
* Viele Calls werden **gar nicht ausgefÃ¼hrt**, obwohl keine Fehlermeldung erscheint 

Der FADELogger zeigt dir in Echtzeit: 

* ğŸ” Wurde `hipLaunchKernel()` wirklich aufgerufen? 
* ğŸ’¥ Wurde Speicher via `hipMalloc()` korrekt alloziert? 
* â›” Wurden Redirects oder Funktionsfehler registriert? 

--- 

## ğŸ› ï¸ ToDo / Erweiterungen 
* [ ] Integration zusÃ¤tzlicher Hooks (`hipGraphLaunch`, `hipStreamWaitEvent`) 
* [ ] Diagramm zur Datenflusskette (CUDAâ†’ZLUDAâ†’HIPâ†’FADE) 
* [ ] Optionale Verbindung zu `strace`, `dlopen`-Tracer oder Symbol-Debugger 
---
